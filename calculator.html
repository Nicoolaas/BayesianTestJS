<HTML>

<style>

body {
  font: 10px sans-serif;
}

.axis path,
.axis line {
  fill: none;
  stroke: #000;
  shape-rendering: crispEdges;
}

.x.axis path {
  display: none;
}

.line {
  fill: steelblue;
  stroke: steelblue;
  stroke-width: 1.5px;
}

</style>

<HEAD>
<TITLE>load demo</TITLE>
</HEAD>

<BODY>



<h1>Bayesian A/B Test Calculator</h1>

<p>Bayesian inference consists in first specifying a prior belief about what effects are likely, and then updating the prior with incoming data.</p>

<p>For example, if our conversion rate is 5%, we may say that it's reasonably likely that a change we want to test could improve that by 5 percentage points --- but that it is most likely that the change will have no effect, and that it is entirely unlikely that the conversion rate will shoot up to 30% (after all, we are only making a small change).</p>

<p>As the data start coming in, we start updating our beliefs. If the incoming data points point at a an improvement in the conversion rate, we start moving our estimate of the effect from the prior upwards; the more data we collect, the more confident we are in it and the further we can move away from our prior. The end result is what is called the posterior --- a probability distribution describing the likely effect from our treatment.</p>

<p>This simple calculator uses the Beta-Bernoulli model (a binary outcome model, where the prior for the success probability is a Beta distribution) applied in the A/B testing context, where the goal of inference is understanding the probability that the test group performs better than the control group.</p>

<h2>Usage</h2>
<ol>
<li>Specify the prior through the alpha and beta parameters of the <a href="http://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>. The parameter values govern two things: the prior success probability (our belief about the average conversion rate, for example) as well as the variance of the prior distribution (small alpha and beta will lead to a prior distribution where success probabilities can vary quite a lot around their mean; large values will lead to a distribution with a small variance). For example, setting alpha to 10 and beta to 10 will give us a prior distribution where the expected success probability is 0.5, but there is a fair amount of uncertainty around that value. Setting them to 100 and 100 will give us the same expected probability of 0.5, but the variance around that value will be much smaller. The <a href="http://en.wikipedia.org/wiki/Probability_density_function">probability density function</a> of the prior will be plotted below.
<li>Have a look at the histogram of success probability differences between the test and control. It expresses prior beliefs about the likely difference of success probabilities between the test and control groups. Because we specified a symmetric prior, the belief is centered around a difference of zero (a priori, A/B tests are just as likely to do worse as they are to do better than the control). If our priors have a low variance, the histogram will put put a low weight on large differences (it is unlikely that a test will do much better or much worse than the control); if the priors have a high variance, large differences will be much more likely.
<li>Gather data!
<li>Input the number of successes (conversions, clicks and so on) and failures in both the test and control groups. This triggers updating the priors with the data.
<li>The prior plots shift to express posterior (prior updated with data) distributions. The density plots will (may) diverge, showing the posterior distributions of the success probability in test and control groups. Similarly, the difference histogram will shift. The part of the distribution lying to the right of zero expresses the confidence that the test performs better; the part to the left that it performs worse.
</ol>





<input id="priorAlpha" value="10" type="number" min="0.1">alpha</input>
<input id="priorBeta" value="10" type="number" min="0.1">beta</input>
<br>
<input id="controlSuccesses" value="0" type="number" min="0">control successes</input>
<input id="controlFailures" value="0" type="number" min="0">control failures</input>
<br>
<input id="testSuccesses" value="0" type="number" min="0">test successes</input>
<input id="testFailures" value="0" type="number" min="0">test failures</input>
<br>
<button id="submit">Submit</button>

<div id="pdfplot"></div>
<div id="histogram"></div>

Test - control mean estimate: <span id="differenceMean">0</span>
Probability that test performs better than control: <span id="testSuccessProbability">0.5</span>

<script charset="utf-8" src="bayes-script.js"></script>

</BODY>
